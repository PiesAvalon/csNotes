{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络的结构 笔记\n",
    "教程视频链接：https://www.bilibili.com/video/BV1hE411t7RN\n",
    "\n",
    "这篇笔记对应视频合集中的\n",
    "- 神经网络-卷积层\n",
    "- 神经网络-最大池化的使用\n",
    "- 神经网络-非线性激活\n",
    "- 神经网络-线性层及其他层介绍\n",
    "\n",
    "本节介绍了运用torch.nn类构建神经网络的流程，对于一些没有介绍到的层，可以参考官方文档。\n",
    "\n",
    "相关的官方文档：https://pytorch.org/docs/stable/nn.html#module-torch.nn\n",
    "\n",
    "卷积、池化、非线性激活是处理数据的方法，在神经网络构建过程中，卷积 -> 池化 -> 非线性激活，三者一般配套使用，下面分别介绍这三种方法。\n",
    "## 1.卷积层（Convolution Layers）\n",
    "模板包括一维卷积、二维卷积等等，处理图片一般使用二维卷积，这里以此为例展开\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建dataset和dataloader\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='../dataset', train=False,download=False, transform=torchvision.transforms.ToTensor())\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入二维卷积模板类\n",
    "from torch.nn.modules import Conv2d\n",
    "\n",
    "#创建神经网络模板子类\n",
    "class Tudui(torch.nn.Module):\n",
    "    #类初始化\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n",
    "        '''\n",
    "        卷积操作对象。常用参数：\n",
    "        stride -> 卷积的步长\n",
    "        padding -> 外层填充量\n",
    "        kernel_size -> 卷积核的大小\n",
    "        in_channels -> 输入数据通道数，黑白为1，彩色为3\n",
    "        out_channels -> 输出的通道数（这个概念已经脱离图像本身了）\n",
    "        '''\n",
    "    \n",
    "    #forward函数，\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个神经网络对象\n",
    "tudui = Tudui()\n",
    "print(tudui)\n",
    "\n",
    "#打印卷积核kernel，由于没有手动设置，在实例化过程中torch自动生成了卷积核\n",
    "print(tudui.conv1.weight.shape)\n",
    "print(tudui.conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    imgs, targets = data#tensor类型的图片\n",
    "    output = tudui(imgs)\n",
    "    print(imgs.shape, output.shape)#输出torch.Size([64, 3, 32, 32]) torch.Size([64, 6, 30, 30])，最后一个batch可能不是64张"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，神经网络的input为tensor格式，其shape为\n",
    "$$inshape = (B, C_{in}, H, W)$$\n",
    "其中：`B`为batch_size，`C`为in_channels，`H`和`W`为图像的长宽。\n",
    "\n",
    "对应的output同样为tensor格式，其shape为\n",
    "$$outshape = (B, C_{out}, H', W')$$\n",
    "其中`C`为out_channels，由卷积核kernel的数量决定， `H'`和`W'`由图片相关参数和卷积操作相关参数决定，即\n",
    "$$H' = \\left\\lfloor\\frac{H + 2 \\times \\text{padding} - K}{\\text{stride}}\\right\\rfloor + 1$$\n",
    "$$W' = \\left\\lfloor\\frac{W + 2 \\times \\text{padding} - K}{\\text{stride}}\\right\\rfloor + 1$$\n",
    "对于卷积、池化等概念的形象理解：https://www.bilibili.com/video/BV1nk4y1271L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用tensorboard展示经过卷积操作的图片\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"..\\logs\")\n",
    "\n",
    "step = 0\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    output = tudui(imgs)\n",
    "    #torch.Size([64, 3, 32, 32]) \n",
    "    writer.add_images(tag='input', img_tensor=imgs, global_step=step)#注意！这里是add_images，不是add_image！\n",
    "    #torch.Size([64, 6, 30, 30])，6通道无法显示，需要reshape（强制转换）\n",
    "    output = torch.reshape(output, (-1, 3, 30, 30))#把多的通道处理为更多的batch，batch_size填-1会自动计算\n",
    "    writer.add_images(tag='output', img_tensor=output, global_step=step)#注意！这里是add_images，不是add_image！\n",
    "    step += 1\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.池化层（Pooling Layers）\n",
    "最大池化可以降低样本的维度，同时保留特征信息，进行该操作可以提升模型的鲁棒性，尤其是面对扭曲和旋转的情况下。此外，池化操作可以减小进入神经网络的数据量，提升数据的信息密度，加快模型训练的速度。pytorch提供了池化层的模板，可以实例化添加到自定义神经网络中。这里用二维最大池化（MaxPool2D）举例，最大池化又被称为下采样（subsampling）。\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tudui2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, ceil_mode=True)\n",
    "        '''\n",
    "        创建最大池化层。常用参数：\n",
    "        dilation -> 空洞池化核（一般不设置）\n",
    "        ceil_mode -> ceiling/floor 指向上或向下取整，此处指池化核覆盖虚空时是否保留\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        return self.maxpool(x)\n",
    "tudui = Tudui2()\n",
    "    \n",
    "#显示输出的结果，效果类似有损压缩\n",
    "writer = SummaryWriter('../logs')\n",
    "step = 0\n",
    "for data in dataloader:\n",
    "    imgs, tags = data\n",
    "    output = tudui(imgs)\n",
    "    writer.add_images(tag='inputs', global_step=step, img_tensor=imgs)\n",
    "    writer.add_images(tag='outputs', global_step=step, img_tensor=output)\n",
    "    step +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化操作输入与输出的tensor数据的shape与卷积操作中类似，不同的是池化中的输出数据通道数$C_{out}$不由kernel数量决定（池化操作不存在多个kernel），而是与输入数据通道数$C_{in}$的值相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[8., 9.],\n",
       "         [7., 6.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在自定义数据上操作时，存在一个数据类型的问题，此时给tensor数据类型加上`dtype=torch.float32`即可解决问题：\n",
    "\n",
    "input = torch.tensor([[[1, 2, 3, 4, 5],\n",
    "                      [6, 7, 8, 9, 0],\n",
    "                      [2, 5, 1, 6, 3],\n",
    "                      [3, 5, 1, 2, 1],\n",
    "                      [7, 3, 5, 3, 6]]])#有的版本的pytorch这样写会报错\n",
    "\n",
    "input = torch.tensor([[[1, 2, 3, 4, 5],\n",
    "                      [6, 7, 8, 9, 0],\n",
    "                      [2, 5, 1, 6, 3],\n",
    "                      [3, 5, 1, 2, 1],\n",
    "                      [7, 3, 5, 3, 6]]], dtype=torch.float32)#如果报错加上这行即可\n",
    "tudui(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.非线性激活（Non-linear Activations）\n",
    "用非线性的激活函数可以给神经网络添加非线性成分，赋予神经网络解决非线性问题的能力。如果没有非线性激活函数，无论神经网络有多少隐含层，都会等效于一个没有隐含层的线性变换。\n",
    "\n",
    "对该问题的说明：https://www.bilibili.com/video/BV1Yh4y1c7sT\n",
    "\n",
    "pytorch提供了对应多种非线性激活函数的非线性激活层模板，可以直接将非线性激活层实例化，添加到神经网络模板子类中。\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "\n",
    "在各种非线性激活函数中，较为常用的有ReLU函数（Rectified Linear Unit Function）：\n",
    "$$\\text{ReLU}(x) = \\text{max}(0, x)$$\n",
    "Sigmoid激活函数（Sigmoid activation function）：\n",
    "$$\\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "下面以ReLU函数为例，说明添加非线性激活层的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Tudui3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu1 = torch.nn.ReLU(inplace=False)\n",
    "        '''\n",
    "        创建非线性激活层。常用参数：\n",
    "        inplace -> 是否替换原有变量，设置为False（默认值）可以保留原有数据，更方便\n",
    "        '''\n",
    "        self.sigmoid = torch.nn.Sigmoid()#sigmoid函数非线性激活，会让图片处理效果更明显\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "tudui = Tudui3()\n",
    "\n",
    "#tensorboard展示输出结果\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('../logs')\n",
    "step = 0\n",
    "for data in dataloader:\n",
    "    imgs, tags = data\n",
    "    output = tudui(imgs)\n",
    "    writer.add_images(tag='inputs', global_step=step, img_tensor=imgs)\n",
    "    writer.add_images(tag='outputs', global_step=step, img_tensor=output)\n",
    "    step +=1\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.归一化层（Normalization Layers）\n",
    "处理输入的tensor数据，使之在不丧失数据原有特征的情况下，均值变为0，方差变为1。这样的处理可以加快神经网络的训练速度。\n",
    "\n",
    "关于归一化层参考视频：https://www.bilibili.com/video/BV1f8411w75v\n",
    "\n",
    "pytorch提供了不同归一化层的模板，这里用`nn.BatchNorm2d`处理图片数据举例。\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Pies(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.BatchNorm2d(num_features=3)\n",
    "        '''\n",
    "        创建归一化层，常用参数：\n",
    "        num_features -> 特征数量，可以理解为输入数据通道数量\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "pies = Pies()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('../logs')\n",
    "step = 0\n",
    "for data in dataloader:\n",
    "    imgs, tags = data\n",
    "    output = pies(imgs)\n",
    "    writer.add_images(tag='inputs', global_step=step, img_tensor=imgs)\n",
    "    writer.add_images(tag='outputs', global_step=step, img_tensor=output)\n",
    "    step +=1\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.线性层（Linear Layers）\n",
    "线性层对一维数据进行线性变换，一般用于展示预测结果（通过线性变换将vector变为scaler）。pytorch为线性层提供了多种模板，这里以一般形式`nn.Linear`模板为例。该过程的公式为\n",
    "$$y = xA^T + b$$\n",
    "其中`A`为weight matrix`B`为bias vector二者均是自动初始化，且在训练过程中逐渐学习到的\n",
    "\n",
    "关于线性层参考视频：https://www.bilibili.com/video/BV1o94y1Y7G9\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Tudui4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=196608, out_features=1)#此处的in_features为数据集中数据展平后的维数\n",
    "        '''\n",
    "        创建线性层。常用参数：\n",
    "        in_features -> 输入数据的维数\n",
    "        out_fetures -> 输出数据的维数\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "tudui = Tudui4()\n",
    "\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    input = torch.flatten(imgs)#torch.flatten可以将多维度tensor数据转换为vector数据\n",
    "    if input.shape[0] == 196608:\n",
    "        output = tudui(input)\n",
    "    print(input.shape, output.shape, output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch250115",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

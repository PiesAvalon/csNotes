{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数与反向传播 笔记\n",
    "教程视频链接：https://www.bilibili.com/video/BV1hE411t7RN\n",
    "\n",
    "这篇笔记对应视频合集中的\n",
    "- 损失函数与反向传播\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.损失函数（Loss Function）\n",
    "Loss Function可以被用于衡量模型的预期输出（output）和目标值（target）之间的误差。Loss Function的计算方式有很多，其中L1Loss采用的是MAE（Mean Absolute Error）计算方式：\n",
    "$$l_n = \\lvert x_n - y_n \\rvert$$\n",
    "$$L = \\{ l_1, l_2, ..., l_N\\}$$\n",
    "再根据`reduction`参数决定对L求和或求均值\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "targets = torch.tensor([1,2, 5], dtype=torch.float)\n",
    "\n",
    "#reshape的目的是添加batch_size这一数据，实际上这个例子中不需要reshape\n",
    "inputs = torch.reshape(inputs, (1, 1, 1, 3))\n",
    "targets = torch.reshape(targets, (1, 1, 1, 3))\n",
    "\n",
    "loss = torch.nn.L1Loss(reduction=\"mean\")#MAE\n",
    "'''\n",
    "实例化loss function。常用参数：\n",
    "reduction -> 若为\"mean\"对L求平均值（MAE），若为\"sum\"，对L求和\n",
    "'''\n",
    "result = loss(inputs, targets)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种常用的计算方式是MSE（mean squared error）：\n",
    "$$l_n = (x_n - y_n)^2$$\n",
    "$$L = \\{ l_1, l_2, ..., l_N\\}$$\n",
    "其余部分与MAE相同\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3333)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mse = torch.nn.MSELoss(reduction=\"mean\")#MAE\n",
    "'''\n",
    "实例化loss function。常用参数：\n",
    "reduction -> 若为\"mean\"对L求平均值（MAE），若为\"sum\"，对L求和\n",
    "'''\n",
    "result_mse = loss_mse(inputs, targets)\n",
    "result_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练分类问题时，也常用到交叉熵（Cross-Entropy Loss）计算\n",
    "\n",
    "关于交叉熵的数学计算，参考：https://www.bilibili.com/video/BV1MP411r7sL\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1019)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0.1, 0.2, 0.3])\n",
    "y = torch.tensor([1])\n",
    "x = torch.reshape(x, (1, 3))\n",
    "loss_cross = torch.nn.CrossEntropyLoss()\n",
    "result_cross = loss_cross(x, y)\n",
    "'''\n",
    "loss_cross的输入x是对于所有类别预测的结果，y是target，即该数据所真实对应的类别\n",
    "'''\n",
    "result_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.反向传播函数\n",
    "Loss Function为更新输出提供依据。即通过Loss Function的返回值可以确定反向传播函数。可以运用反向传播函数求神经网络中神经元的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root = \"../dataset\", train=False, download=False, transform=torchvision.transforms.ToTensor())\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=1, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tudui2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(kernel_size=5, padding=2, in_channels=32, out_channels=64),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(in_features=1024, out_features=10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "tudui = Tudui2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    output = tudui(imgs)\n",
    "    print(output)\n",
    "    print(targets)\n",
    "\n",
    "    result_loss = loss_cross(output, targets)\n",
    "    print(result_loss)\n",
    "\n",
    "    result_loss.backward()\n",
    "    #运行backwards方法，会在神经网络中计算每个神经元权重的梯度（gradient)并添加到神经网络的属性中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.优化器（optimizer）\n",
    "优化器可以利用梯度，对神经网络进行操作，起到降低误差的目的。优化器的类在`torch.optim`中，创建优化器需要对其进行实例化。\n",
    "\n",
    "官方文档：https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "pytorch提供的优化器种类有很多，这里以SGD（Stochastic Gradient Descent，随机梯度下降）优化器举例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudui = Tudui2()\n",
    "\n",
    "optim = torch.optim.SGD(params=tudui.parameters(), lr=0.001) \n",
    "'''\n",
    "创建优化器。常用参数：\n",
    "params -> 形参，调用神经网络方法即可获得\n",
    "lr -> 学习率，一般先设置的较搞进行学习，再进行调整\n",
    "'''\n",
    "for epoch in range(20):\n",
    "    running_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = tudui(imgs)\n",
    "        #初始化梯度为0\n",
    "        optim.zero_grad()\n",
    "        #获取loss值\n",
    "        result_loss = loss_cross(outputs, targets)#loss function不一定是单一的batch\n",
    "        #用loss值确定的反向传播函数获取梯度\n",
    "        result_loss.backward()\n",
    "        #用优化器进行参数优化\n",
    "        optim.step()\n",
    "        running_loss += result_loss\n",
    "    print(running_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch250115",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
